---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: vllm-qwen-server
  namespace: genai
  annotations:
    kubernetes.io/pvc-protection: "false"  
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 900Gi
  storageClassName: gp3
  volumeMode: Filesystem
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-qwen-server
  namespace: genai
  labels:
    app: vllm-qwen-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-qwen-server
  template:
    metadata:
      labels:
        app: vllm-qwen-server
    spec:
      securityContext:
        seccompProfile:
          type: RuntimeDefault
      automountServiceAccountToken: false
      volumes:
      - name: cache-volume
        persistentVolumeClaim:
          claimName: vllm-qwen-server
      - name: hf-secret-volume
        secret:
          secretName: hf-secret
      # vLLM needs to access the host's shared memory for tensor parallel inference.
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: "32Gi"
      containers:
      - name: vllm-qwen-server
        # This is vllm/vllm-openai:v0.9.0.1 sha digest
        image: vllm/vllm-openai@sha256:6e128f5e60fcb8b8ca76eb63f102c0d96c34a7ef4ff014df920eb3eb70dd9193
        imagePullPolicy: Always
        securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - NET_RAW
            seccompProfile:
              type: RuntimeDefault
        resources:
          requests:
            cpu: "22"
            memory: 64Gi
            nvidia.com/gpu: 2
          limits:
            cpu: "22"
            memory: 64Gi
            nvidia.com/gpu: 2
        args:
        - --model=$(MODEL_ID)
        - --enable-auto-tool-choice
        - --trust-remote-code
        - --max_num_batched_tokens=8192
        - --tool-call-parser=hermes
        - --max-num-seqs=8
        - --max_model_len=8192
        - --dtype=bfloat16 
        - --tensor-parallel-size=2
        - --gpu-memory-utilization=0.90
        env:
        - name: OMP_NUM_THREADS
          value: "12"  
        - name: MODEL_ID
          value: Qwen/Qwen3-14B
        ports:
        - containerPort: 8000  
        volumeMounts:
        - mountPath: /root/.cache/huggingface
          name: cache-volume
        - name: shm
          mountPath: /dev/shm
        - mountPath: /secrets
          name: hf-secret-volume
          readOnly: true
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 240
          periodSeconds: 10
          failureThreshold: 40
          successThreshold: 1
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 240
          periodSeconds: 10
      tolerations:
        - key: nvidia.com/gpu
          value: "true"
          effect: "NoSchedule"
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-qwen-server
  namespace: genai
spec:
  # The label selector should match the deployment labels & it is useful for prefix caching feature
  selector:
    app: vllm-qwen-server
  ports:
  - name: http-vllm-qwen-server
    port: 8000
    protocol: TCP
    targetPort: 8000
  sessionAffinity: None
  type: ClusterIP 